---
title: "How to work with netcdf?"
format: 
  html:
    page-layout: full
    title-block-banner: true
toc: true
toc-expand: 2
execute:
  eval: false
---

How I use this document: I click the tab I want (R or python), and control-F to find the operation I'm looking for.

<details>
  <summary>Click here for optional background</summary>

  Climate model output is often represented as a three-dimensional spatiotemporal data cube -- we work with 2-dimensional maps representing a single point in time, so that looking at multiple time points gives us a 3d cube^[This is frequently extended to higher dimensions, e.g., if our climate variable is recorded at different elevations. We're not going to worry about those right now]. This data is frequently recorded in the `netCDF` file format, which end in file extension `.nc`. One nice feature of NetCDFs is that they are self-contained, as they include all of the relevant metadata inside of the file itself. Another nice feature is that you can easily access arbitrary subsets of the data, for convenient lazy loading. Python has great, straightforward support for netCDF -- the community has really rallied around the `xarray` package, and `xarray` was pretty much designed for netCDFs.



In R, things are a little more complicated. A lot of the support for spatiotemporal data in R doesn't come from people working with netCDFs, but from different communitities (e.g. demographers, remote sensing people, everyone that loves GIS). As a result, the popular spatiotemporal packages are designed for super general purpose use. We can almost certainly do all the same netCDF-based analysis as in python, but the documentation for netCDF operations is not as nice. 

Another complicating aspect with working with netCDF in R is that there are multiple packages out there to use, and the ones being actively developed are relatively new. Here's a quick rundown: 

- `terra` is the most general and most powerful package, and it's what I prefer to use in R. It's built as an alternative to the old `raster` package, which used to be popular back in the day.

- `stars` is made more specifically for "data cubes". Probably the closest philisophically aligned to xarray, but it's more general than xarray (in particular, in its abiltiy to handle weird curved grids). 

- `tidync` is made specifically for netCDF files, and it does a good job at picking out the grid of data that we want It's really good at what it does (reading in a netCDF file and taking subsets of it), but the package doesn't do much more than that. As the name suggests, I think this tool is best if your data fits in memory in "long" format (i.e. a dataframe with 1 row per (lat, lon, time)). 

In summary: I think `terra` is the best package for our purposes (i.e working with nicely gridded netCDF). `stars` is cool but is missing a bunch of functionality and documentation (as of this writing). `tidync` is amazing at what it does (i.e. identifying dimensions / lazy filtering), but is too barebones for analysis. It's also worth mentioning that `stars` and `tidync` sometimes make use of the `cubelyr` package, which isn't actively maintained (although it's quite nice).  It's also worth mentioning the low-level packages that are often working under the hood: `sf` is a general spatial package in R that gives us the data class we often store our objects in, and `RNetCDF` is a really low-level package for reading in netcdf data, and `ncdf4` is a little more high level and is still useful for grabbing things like metadata.


  
</details>


## Preliminaries

To start, let's load a bunch of packages and get some utilities

::: {.panel-tabset group="language"}


## R

```{r}
# global utils
library(tidyverse)
library(here)
library(rnaturalearth)

# specific R utils
library(tidync)
library(stars)
library(terra)

# helper function
lon_to_180 <- function(lon360){
  ifelse(lon360 > 180, lon360 - 360, lon360)
}

# example data
coasts <- ne_coastline(returnclass = 'sf')
```

## Python

```{python}
#| code-fold: true

import xarray as xr
import matplotlib.pyplot as plt
import urllib
import gzip
import xesmf as xe # need to get this one from conda-forge. not pip!
import cartopy.crs as ccrs

import tastymap # a way to quickly make discrete colorbars :)
reds_discrete = tastymap.cook_tmap("cet_CET_L18", num_colors=10).cmap

```

:::

Let's grab some netCDF files to play with. For demonstration purposes, we'll use a land temperature product -- NASA GISSTEMP (qfe is a quality control adjusted version).  I'm just going to download this one using good old fashioned wget

Let's grab two more files -- two decades of monthly data from a CESM2 historical run, accessed from [here](https://rda.ucar.edu/). Again pulling 2m air temperature. 

(aside: I find this cesm [data dictionary](https://www.cesm.ucar.edu/community-projects/lens2/output-variables) extremely helpful)


::: {.panel-tabset group="language"}


## R

```{r}
#| eval: false
#| code-fold: true

# GISTEMP ------------------------------------------------------
gistemp_url <- 'https://data.giss.nasa.gov/pub/gistemp/gistemp250_GHCNv4.nc.gz'

# downloads into the working directory
download.file(gistemp_url, destfile = here('ghcn_demo.nc.gz'))
# unzip the file
R.utils::gunzip(here('ghcn_demo.nc.gz'), remove = FALSE)
ghcn_path <- here('ghcn_demo.nc')


# CESM ----------------------------------------------------------

# 2010-2014
cesm_url <- 'https://data-osdf.rda.ucar.edu/ncar/rda/d651056/CESM2-LE/atm/proc/tseries/month_1/TREFHT/b.e21.BHISTcmip6.f09_g17.LE2-1001.001.cam.h0.TREFHT.201001-201412.nc'
download.file(cesm_url, destfile = here('cesm_demo.nc'), method = 'wget')

# 2000-2009
cesm_url2 <- 'https://data-osdf.rda.ucar.edu/ncar/rda/d651056/CESM2-LE/atm/proc/tseries/month_1/TREFHT/b.e21.BHISTcmip6.f09_g17.LE2-1001.001.cam.h0.TREFHT.200001-200912.nc'
download.file(cesm_url2, destfile = here('cesm_demo2.nc'), method = 'wget')


cesm_paths <- c(here('cesm_demo.nc'), here('cesm_demo2.nc'))


```

## Python


```{python}
# GISTEMP ------------------------------------------------------
gistemp_url = 'https://data.giss.nasa.gov/pub/gistemp/gistemp250_GHCNv4.nc.gz'


# downloads ghcn_demo.nc into the working directory
gistemp_response = urllib.request.urlopen(gistemp_url)
compressed_file = gistemp_response.read()
with open("ghcn_demo.nc", mode="wb") as file:
    file.write(gzip.decompress(compressed_file))

# CESM ----------------------------------------------------------

# downlaods cesm_demo.nc and cesm_demo2.nc to working directory

# 2010-2014
cesm_url = 'https://data-osdf.rda.ucar.edu/ncar/rda/d651056/CESM2-LE/atm/proc/tseries/month_1/TREFHT/b.e21.BHISTcmip6.f09_g17.LE2-1001.001.cam.h0.TREFHT.201001-201412.nc'
cesm_response = urllib.request.urlopen(cesm_url).read()
with open("cesm_demo.nc", mode="wb") as file:
    file.write(cesm_response)


# 2000-2009
cesm_url2 = 'https://data-osdf.rda.ucar.edu/ncar/rda/d651056/CESM2-LE/atm/proc/tseries/month_1/TREFHT/b.e21.BHISTcmip6.f09_g17.LE2-1001.001.cam.h0.TREFHT.200001-200912.nc'
cesm_response2 = urllib.request.urlopen(cesm_url2).read()
with open("cesm_demo2.nc", mode="wb") as file:
    file.write(cesm_response2)

```


:::


# Reading files

Let's start with the cesm file for our demo, assuming that there's a single variable that we're interested in, TREFHT. Compare to the output after reading in the GISTEMP data.


## Reading in a single netcdf

::: {.panel-tabset group="language"}

## R


```{r}
# terra
cesm_terra <- rast(cesm_paths[1], subds = 'TREFHT')
gistemp_terra <- rast(ghcn_path, subds = 'tempanomaly')


## this might be equivalent?
# cesm_terra <- rast(cesm_path)
# cesm_terra_TREFHT <- subset(cesm_terra, 'TREFHT')


# stars
cesm_stars <- read_ncdf(cesm_paths[1], var = 'TREFHT', proxy = F)
gistemp_stars <- read_ncdf(ghcn_path, var = 'tempanomaly', proxy = F)


# tidync
cesm_tnc <- tidync(cesm_paths[1])
gistemp_tnc <- tidync(ghcn_path)


```


## Python 

```{python}
gistemp_path = "climate-notebooks/gistemp250_GHCNv4.nc"
gistemp_da = xr.open_dataset(gistemp_path)
gistemp_da = gistemp_da['tempanomaly']

cesm_path1 = 'climate-notebooks/cesm_demo.nc'
cesm1_da = xr.open_dataset(cesm_path1)
cesm1_da = cesm1_da['TREFHT']

cesm_path2 = 'climate-notebooks/cesm_demo2.nc'
cesm2_da = xr.open_dataset(cesm_path2)
cesm2_da = cesm2_da['TREFHT']
```

:::

## Reading multiple netcdf

Large netCDFs are often split up into multiple files, with each file representing a time chunk. For example, in the CESM files downloaded above, one file represents monthly output for 2000-2010, and the other file represents monthly output for 2010-2014. The two files live on identical grids, so it makes sense to combine them into a single object. (if the two files live on different grids, @sec-regridding might be useful)

::: {.panel-tabset group="language"}

## R

```{r}

## terra

all_cesm <- rast(cesm_paths) # recall, cesm_paths is a vector containing two files

# if we already have two rasters read in, we can combine them
cesm1 <- rast(cesm_paths[1], subds = 'TREFHT')
cesm2 <- rast(cesm_paths[2], subds = 'TREFHT')
all_cesm_alt <- c(cesm1, cesm2)


## stars

all_cesm_stars <- read_stars(cesm_paths)



```

## Python

```{python}
cesm_mf = xr.open_mfdataset('climate-notebooks/cesm_demo*.nc')
cesm_mf = cesm_mf['TREFHT']
```

:::

# Looking at metadata

e.g. finding the units of a variable

::: {.panel-tabset group="language"}

## R

I like the `ncmeta` package for this in R, but each R package has their own way of doing this.

```{r}


# terra

units(cesm_terra)
time(cesm_terra)
varnames(cesm_terra)
longnames(cesm_terra)
metags(cesm_terra)


# ncmeta

ncmeta::nc_att(path)
print(ncdf4::nc_open(path))

# tidync

hyper_vars()
nc_get()

# stars


```

## Python

```{python}
cesm1_da.attrs
cesm1_da.units
cesm1_da.attrs['units']

cesm1_da.long_name
```

:::

# Preprocessing

## Switching between (-180, 180) and (0, 360) lon

Climate model data usually formats longitudes to be between (0, 360), but observations are typically use (-180, 180).

Below is code to just do this reshift. This is easy for point data (e.g climate model output), because we can just shift the longitude values over. But for polygons (e.g. adding coastlines), this is tricky, because we need to make sure we respect the prime meridian when we shift over.

In general, my recommendation is to move the climate model output to (-180, 180), NOT move the observational data to (0, 360).

::: {.panel-tabset group="language"}

## R 

If you want more complex reprojections, see [here](https://r.geocompx.org/reproj-geo-data)

```{r}
# terra
cesm180 <- rotate(cesm_terra)  # recall: cesm_terra is in (0, 360)


## BONUS: an alternate approach using just sf ---------------

# pak::pkg_install('lwgeom') 
library(lwgeom) # useful for st_wrap_x, https://github.com/r-spatial/sf/issues/2058

# example data, which is in (-180, 180)
coasts <- ne_coastline()
plot(st_geometry(x))

# Shift from -180 - 180 to 0 - 360
coasts360 <- st_wrap_x(coasts, 0, 360)
plot(st_geometry(coasts360))

# Shift from 0 - 360 to -180 - 180
# cutline at x = 180
# move things to the left of the cutline by 360
# then move everything left by 360
coasts180 <- st_wrap_x(coasts360, 180, 360)
st_geometry(coasts180) <- st_geometry(coasts180) - c(360, 0)
plot(st_geometry(coasts180))



## bonus bonus: I think these are also supposed to work, but I couldn't figure it out. -----------

# coasts180 <- st_wrap_dateline(coasts360, options = c("WRAPDATELINE=YES", "DATELINEOFFSET=180"))
# coasts180 <- st_shift_longitude(coasts360)

```

## Python

```{python}
# 360 to 180
cesm_180 = cesm_mf.copy()
cesm_180.coords['lon'] = (cesm_180.coords['lon'] + 180) % 360 - 180
cesm_180 = cesm_180.sortby(cesm_180.lon)

# 180 to 360
lon360 = np.mod(cesm_180["lon"], 360)
cesm_360 = cesm180.assign_coords(lon=lon360).sortby("lon")
```

:::

## Datetime manipulation

::: {.panel-tabset group="language"}

## R

In R, lubridate makes this dead simple, so I don't think this needs to be included

## Python

I find working with datetimes in python/xarray really confusing. I'm never really sure when to use cftime vs pandas vs numpy.

One difference between R and python that I've found a bit annoying is that it's difficult to specify just year-month in a monthly xarray -- we always need to specify the full date, at least down to the day. The function below transforms a monthly dataarray by resetting the day index to 1, and resetting the time index to 00

```{python}
def clean_monthly_da(da):
  '''
  takes a monthly-indexed xarray object and resets the day-time component
  da should have dimension "time". The day and time should be meaningless.
  returns a dataarray of the same dimension, just with the time coordinate changed.
  '''
  years = da.time.dt.year
  months = da.time.dt.month

  new_times = [pd.Timestamp(f'{y}-{m}-01') for y, m in zip(years, months)]

  da['time'] = new_times

  return da


```

:::




## Shifting Southern Hemisphere by 6 months

Scenario: we have a global xarray object and we want to obtain a global "summer" average. Summer in the Northern and Southern hemispheres are offset by 6 months. The chunk below shifts all the southern hemisphere data back by 6 months.

::: {.panel-tabset group="language"}

## Python

```{python}
## if the data is monthly resolution, then this trick works
# this doesn't work for different temporal resoltuions bc shift only works by index and not by time
xr.where(eg_xr.lat > 0, era_temp, eg_xr.shift(time = -6))


## a first pass solution which works for data at all temporal resolutions
# NOTE: wait this might not work sometimes bc of the dependence on np.timedelta. 
# TODO:I should revisit this
eg_nh = eg_xr.where(lat>=0, drop = True)
eg_sh = eg_xr.where(lat<0, drop = True)
eg_sh['time'] = eg_sh['time'] - np.timedelta64(6,'M')
eg_shifted = xr.combine_by_coords([eg_nh, eg_sh])

## Annother attempt at a solution.
## I think this is a more foolproof approach?
def shift_sh(x):
    """
    shift time in the southern hemisphere by 6 months, to match seasonality with nh
    x is an xarray object with dimension "time" and "lat", whose temporal resolution is monthly or finer
    """
    is_nh = x.lat >= 0

    x_nh = x.where(is_nh, drop=True)
    x_sh = x.where(~is_nh, drop=True)

    new_sh_time = [
        cftime.DatetimeNoLeap(
            t.year + (6 - 1) // 12,
            (t.month + 6 - 1) % 12 + 1,
            t.day,
            calendar=t.calendar,
        )
        for t in x_sh.time.values
    ]

    x_sh["time"] = new_sh_time
    x_sh = x_sh.sortby('time')
    x_shifted = xr.combine_by_coords([x_nh, x_sh], join='inner')

    return x_shifted


```

## R

```{r}
#todo
```

:::


## Detrending  {#sec-detrending}

Detrending commonly refers to removing a linear time trend.

::: {.panel-tabset group="language"}

## R

```{r}
# terra

## method 1
poly_coef <- regress(cesm_terra, 1:nlyr(cesm_terra), formula = y~x)$x
time_predictor_rast <- rast(cesm_terra) # empty rast with the right dims
for (i in 1:nlyr(cesm_terra)) {
  time_predictor_rast[[i]] <- as.numeric(time(cesm_terra)[i])
}
fitted_vals <- poly_coef[['(Intercept)']] +
  poly_coef[['x']] * time_predictor_rast

no_trend <- cesm_terra - fitted_vals


## method 2: I think slower, but I'm not sure

#' x and y are both vectors of equal length, representing time series
detrend_cell <- function(rast_cell, x){
  # for masked areas, just return NA
  if (sum(!is.na(y_values)) < 2) {
    return(NA)
  }
  
  mod <- lm(y,x)
  fitted_vals <- predict(mod)  
  fitted_vals
}

no_trend <- app(cesm_terra, \(y) detrend_cell(y, x = 1:nlyrs(cesm_terra)))


# eg_trend <- x * poly_coef # does this work?
# eg_trend <- predict(poly_coef) # i doubt this works, but worth a shot.


```

## Python

```{python}
poly_coef = eg_xr.polyfit(dim='time', deg=1)['polyfit_coefficients']
eg_trend = xr.polyval(eg_xr.time, poly_coef)
eg_anoms = eg_xr - eg_trend
```

:::

## Removing the seasonal cycle

### Method 1: by removing climatological means 

Shown here for monthly data, but similar code works for different timescales

::: {.panel-tabset group="language"}

## R

```{r}
# terra

monthly_climatology = tapp(eg_xr, 'months', "mean")
```

## Python

```{python}
monthly_climatology = eg_xr.groupby("time.month").mean()
eg_xr_anom = (
    eg_xr.groupby("time.month") - monthly_climatology
)
```

:::


### Method 2: by fourier basis

The climatology estimated by Method 1 will not be a smooth curve, because it's based on the sample mean. The code below adds a step to method 1, smoothing out the climatology via fourier transform. We then compute anomalies by subtracting out the smoothed climatology.

::: {.panel-tabset group="language"}

## R


```{r}
# todo
```


## Python


```{python}
def fourier_climatology_smoother(da, n_time, n_bases=5):
    """
    taken from karen's code

    calculates a fourier-smoothed climatology at each gridcell, using n_bases components
    output is an xarray data array with climatologies, with dimension (n_time, lon, lat)

    da is a data array, with dimensions (time, lon, lat)
    n_time is 365 if removing the doy climatology or 12 if removing the monthly climatology
    nbases is the number of fourier components we want to use
    """
    # create basis functions to remove seasonal cycle
    time = np.arange(1, n_time + 1)
    t_basis = time / n_time

    # list of the first n_bases fourier components
    bases = np.empty((n_bases, n_time), dtype=complex)
    for counter in range(n_bases):
        bases[counter, :] = np.exp(2 * (counter + 1) * np.pi * 1j * t_basis)

    if n_time == 365:
        # get empirical average for the doy
        empirical_sc = da.groupby("time.dayofyear").mean()  # dim (doy, lat, lon)
        mu = empirical_sc.mean(
            dim="dayofyear"
        )  # map of average across all days. dim (lat, lon)
    elif n_time == 12:
        # get empirical average for the month
        empirical_sc = da.groupby("time.month").mean()  # dim (month, lat, lon)
        mu = empirical_sc.mean(
            dim="month"
        )  # map of average across all days. dim (lat, lon)
    else:
        raise ValueError("only n_time = 12 or 365 are handled")

    nt, nlat, nlon = empirical_sc.shape
    loc_len = nlat * nlon

    # project zero-mean data onto basis functions
    data = (empirical_sc - mu).data

    coeff = 2 / n_time * (np.dot(bases, data.reshape((nt, loc_len))))

    # reconstruct seasonal cycle
    rec = np.real(np.dot(bases.T, np.conj(coeff)))
    rec = rec.reshape((nt, nlat, nlon))

    # add back the mean
    da_rec = empirical_sc.copy(data=rec) + mu
    return da_rec


# example usage
eg_climatology = fourier_climatology_smoother(eg_xr, n_time = 12, n_bases = 5)
eg_xr_anom = eg_xr.groupby('time.month') - eg_climatology

```

:::


## Regridding {#sec-regridding}

Going coarser, e.g going from 1 degree (CESM LENS) to 2 degree (NASA GISTEMP)

::: {.panel-tabset group="language"}

## R

```{r}
# terra

coarse_terra <- resample(cesm_terra, gistemp_terra, method = 'average')
# aggregate() good if the lower resolution is a multiple of the larger


# stars
coarse_stars <- st_warp(src = cesm_stars, dest = gistemp_stars)


```

## Python


```{python}
# using interp_like(), which is suitable for "easier" problems
# i.e. going between similar grids

# make sure that the only dimensions in gistemp_xr are "lon" and "lat"
coarse_xr = cesm_xr.interp_like(gistemp_xr, method="bilinear")


# using xesmf ------------
# this works even for going from curvilinear tracer grids (like cesm sst) <-> to rectangular grids (like cesm tas)

gistemp_regridder = xe.Regridder(cesm_xr, gistemp_xr, "bilinear") 
coarse_xr = gistemp_regridder(cesm_xr["tas"])


```

:::

going finer, e.g. going from 2 degree (GISTEMP) to 1 degree (CESM)

::: {.panel-tabset group="language"}

## R

```{r}

# terra
fine_terra <- resample(gistemp_terra, cesm_terra, method = 'bilinear')

# stars
fine_stars <- st_warp(src = gistemp_stars, dest = cesm_stars, use_gdal = T, method = 'bilinear')

```

## Python


```{python}
# xesgf has some other really cool regridding options
cesm_regridder = xe.Regridder(gistemp_xr, cesm_xr, "bilinear") 
fine_xr = cesm_regridder(gistemp_xr["tas"])

```

:::

<!-- ## Reprojection to different coordinates {#sec-reprojection}

Suppose you want to combine two rasters, but they're in different coordinate systems. Reprojection lets you change the coordinate system.


::: {.panel-tabset group="language"}

## R

```{r}
# terra
st_transform(terra::crs(rast))
# or resample?


```

## Python

```{python}

# todo
```

::: -->

## Area weighting

::: {.panel-tabset group="language"}

## R

```{r}

# terra

expanse(unit = 'km') #

# old raster method  https://stackoverflow.com/a/55233039
# try to adapt this to terrra
r <- abs(init(raster(), 'y'))
s <- stack(r, r, r)
a <- area(s) / 10000
y <- sm * a
weighted_average <- cellStats(y, sum) / cellStats(a, sum)




# stars

# st_area() # 


```

## Python


```{python}
# todo: make sure this works with the actual example.

weights_cos = np.cos(np.deg2rad(cesm_eg['lat']))
cesm_eg_weighted = cesm_eg.weighted(weights_cos)
cesm_eg_weighted.mean(("lon", "lat"))
```

:::


# Subsetting


## Pulling out a single pixel

::: {.panel-tabset group="language"}

## R

```{r}
la_lonlat <- c(-118.24368, 34.05223)

# tidync
la_tnc <- cesm_tnc %>%
  hyper_filter(lat = index == which.min(abs(lat - la_lonlat[2])),
               lon = index == which.min(abs(lon_to_180(lon) - la_lonlat[1])))

# stars

# a <- tibble(lon = la_lonlat[1], lat = la_lonlat[2])
# la_sf <- st_as_sf(a, coords = c('lon', 'lat'))

la_sfc <- st_point(la_lonlat) %>%
  st_sfc(crs = st_crs(cesm_stars))

# bilinear = F does nearest neighbor
la_stars <- cesm_stars %>%
  st_extract(la_sfc, bilinear = F) 


# terra

terra::extract(raster, points) # maybe points needs to be a spatVector? (via vect(points))

# cesm_terra[1] # pull outs the first gridcell


```

## Python

```{python}
la_lonlat = [-118.24368, 34.05223]

eg_xr.sel(lon=la_lonlat[0], lat=la_lonlat[1], method='nearest')
```

:::

## Pulling out a single year

::: {.panel-tabset group="language"}

## R

```{r}
# tidync

cesm_tnc %>%
  hyper_filter(
    time = year(time) == 2010
  )

# stars

# dplyr way (requires cubelyr)
year_stars1 <- cesm_stars %>%
  filter(year(time) == 2010)

# "base r" way
time_vals <- st_get_dimension_values(cesm_stars, 'time') %>%
  ymd()
ind_2010 <- which(year(time_vals) == 2010)
year_stars2 <- cesm_stars[,,,ind_2010]


# terra

cesm_terra %>%
  subset(year(time(.)) == 2010)

cesm_terra[[year(time(cesm_terra)) == 2010]] # alt


# cesm_terra[[1]] # pulls out the first month
```

## Python

```{python}
eg_xr.sel(time=eg_xr['time.year'] == 2010)
# eg_xr.sel(time=slice("2000", "2010")) # a subset of years
```

:::

## Region masking

Suppose you have a polygon (or shapefile, ect) that you want to subset by.

::: {.panel-tabset group="language"}

## R 

```{r}
# using the US as an example.
usa <- ne_countries(country = 'United States of America')
usa360 <- st_wrap_x(usa, 0, 360)



# stars
usa_stars <- cesm_stars %>%
  st_crop(usa360)

# terra

terra::crop(SpatRaster, sf)

# or extract?

# or mask?

countries <- geodata::world(resolution = 5, path = "maps") # get land map
terra::mask(cesm_terra, countries)

```

## Python


```{python}
import regionmask

def get_landmask(x):
  # create a landmask, which still includes antactica and greenland
  land = regionmask.defined_regions.natural_earth_v5_0_0.land_110
  landmask = land.mask(eg_xr)  # ocean is nan, land is 0
  is_land = landmask == 0

  # we also generally wanna get rid of greenland
  greenland = regionmask.defined_regions.natural_earth_v5_0_0.countries_110[["Greenland"]]
  greenland_mask = greenland.mask(eg_xr)
  is_not_greenland = greenland_mask.isnull() # greenland is 0, everything else is nan

  # we also generally wanna get rid of antarctica
  is_not_antarctic = eg_xr["lat"] > -60

  # apply all masks
  eg_xr_land = eg_xr.where(is_land & is_not_greenland & is_not_antarctic)

  return eg_xr_land

eg_xr_land = get_landmask(eg_xr)

```

:::


## Subsetting based on a lon/lat box

Suppose we're interested in the southwestern US, maybe defined as the box between 124-105E and 32-45N


::: {.panel-tabset group="language"}

## R

```{r}

# tidync
sw_tnc <- cesm_tnc %>%
  hyper_filter(
    lon = lon >= 105 & lon <= 124,
    lat = lat >= 32 & lat <= 45
  )

# stars

sw_box <- c(xmin= 105, xmax = 124, ymin = 32, ymax = 45)
sw_bbox <- st_bbox(sw_box, crs = st_crs(cesm_stars))

sw_stars <- st_crop(cesm_stars, sw_bbox)
## check our work
# st_get_dimension_values(sw_stars, 'lon')
# st_get_dimension_values(sw_stars, 'lat')

# terra

r1 <- crop(cesm_terra, ext(-50,0,0,30))

# cesm_terra[,1], pulls out a single latitude (a lon x time matrix)
# cesm_terra[1,] pulls out a single longitude (a lat x time matrix)
```

## Python


```{python}

is_in_lat = eg_xr['lat'] >= 32 & eg_xr['lat'] <= 45
is_in_lon = eg_xr['lon'] >= 105 & eg_xr['lon'] <= 124

is_in_box = is_in_lat & is_in_lon

# apply landmask
cesm_box = eg_xr.where(is_in_box)

```

:::

# Averaging 

## Averaging over time 

(i.e. get one map with the temporal average)

::: {.panel-tabset group="language"}

## R

```{r}
# stars

timeavg_stars <- cesm_stars %>%
  st_apply(c('lon', 'lat'), mean, na.rm = T)


# terra

# each time point is a LAYER
# and terra by default takes the layer-wise mean
mean(eg_aod)


# cesm_terra %>%
#   app()

# tidync (i.e. with the raw data cube, which would also work with stars)
cesm_tnc %>%
  hyper_tbl_cube() %>%
  group_by(lat, lon) %>%
  summarize(avg_map = mean(TREFHT))


```


## Python

```{python}
eg_xr.mean('time')
```

:::

## Averaging over space 

(i.e. get a single time series with the spatial average)

::: {.panel-tabset group="language"}

## R

```{r}

# stars

spaceavg_stars <- cesm_stars %>%
  st_apply('time', mean, na.rm = T)


# terra
cesm_terra %>%
  global('mean', na.rm = T) %>%
  mutate(time = ymd(time(cesm_terra))) 


# tidync (i.e. with the raw data cube, which stars also supports in backend)
cesm_tnc %>%
  hyper_tbl_cube() %>%
  group_by(time) %>%
  summarize(avg_ts = mean(TREFHT))

```

## Python

```{python}
eg_xr.mean(dim=['lat', 'lon'])
```

:::

## Monthly data -> Seasonal average

Say, we want one value per year -- the June - August mean.

::: {.panel-tabset group="language"}

## R

```{r}
jja_months <- c(6,7,8)

# stars

spaceavg_stars <- cesm_stars %>%
  filter(month(time) %in% jja_months) %>%
  st_apply('time', mean, na.rm = T)


# terra

# https://stackoverflow.com/questions/73035913/r-computing-seasonal-raster-based-on-specified-months-in-r
# tapp()

spaceavg_terra <- cesm_terra %>%
  subset(month(time(.)) %in% jja_months) %>%
  mean()


# tidync (i.e. with the raw data cube, which stars also supports in backend)


# might need to self-implement mutate for this... or check out stars mutate.
cesm_tnc %>%
  hyper_tbl_cube() %>%
  filter(month(time) %in% jja_months) %>%
  group_by(year(time)) %>%
  summarize(avg_jja = mean(TREFHT))




```


## Python

```{r}
from xarray.groupers import SeasonGrouper

ds.groupby(time=SeasonGrouper(["DJF", "MAMJ", "JJA", "SON"])).mean() # calculates each seasonal mean.


# This works a little better, by accounting for the number of days in the month. This needs to be tested.

month_length = ds.time.dt.days_in_month
season_grouper = xr.groupers.SeasonGrouper(["DJF", "MAMJ", "JJA", "SON"])
weighted_sum = (ds * month_length).groupby(time=season_grouper).sum()
sum_of_weights = month_length.groupby(time=season_grouper).sum()
weighted_avg = weighted_sum / sum_of_weights


# More generally, if we want to subset to JJA:
ds_jja = ds.where(ds['time.month'].isin([6,7,8]), drop = True)


# dirty way of getting annual JJA max ts in northern heimpshere, DJF in southern hemipshere
# with DJF using the year of the JF

from xarray.groupers import SeasonResampler
import cftime

nh_jja = (
    ds.sel(lat=slice(0, None))
    .groupby(time=SeasonResampler(["JJA"]))
    .max()
)
# remove month/day info, keeping only year
new_nh_time = [
    cftime.DatetimeNoLeap(
        t.year,
        1,
        1,
        calendar=t.calendar,
    )
    for t in nh_jja.time.values
]
nh_jja['time'] = new_nh_time
sh_djf = (
    ds.sel(lat=slice(None, 0))
    .groupby(time=SeasonResampler(["DJF"], drop_incomplete=True))
    .max()
)
# mark the year as the year of the jan and feb
# and everything else as 1 1 (to match ouptut of hdp package)
new_sh_time = [
    cftime.DatetimeNoLeap(
        t.year + 1,
        1,
        1,
        calendar=t.calendar,
    )
    for t in sh_djf.time.values
]
sh_djf["time"] = new_sh_time
sh_djf = sh_djf.dropna(dim="time", how="all")  # drop the last year bc no data

season_max = xr.combine_by_coords([nh_jja, sh_djf])
```

:::


# Plotting

## Plotting a single map

::: {.panel-tabset group="language"}

## R

I'm extremely partial to `ggplot` for plotting, so all of these examples are ggplot based.

```{r}

# terra
library(tidyterra)
ggplot() + 
  geom_spatraster(data = timeavg_terra) + 
  scale_fill_viridis_c() +
  # + coord_sf(xlim = c(105, 124), ylim = c(32, 45), expand = FALSE) # bonus: zoom into a latlon box



# stars

ggplot() +
  geom_stars(data = timeavg_stars) +
  scale_fill_viridis_c() + 
  geom_sf(data = x360) + 
  coord_sf()  
  

## Bonus! How to zoom into a lon-lat box:




```


## Python

```{python}
# also worth looking at hvplot!
eg_xr.mean('time').plot()
```


::: 

## Plotting in python with hvplot

I've been playing around with `hvplot` (via the accessor `hvplot.xarray`), and I've really been enjoying it as a way to quickly prototype figures. I see two main advantages compared to using `matplotlib`. First, composing plots is wayyyyyy easier -- if I have two figures, I can put them side by side just by typing `fig_a + fig_b`. I've come to love this type of plot composition from the `patchwork` package in `R`, so I'm glad that `hvplot` has similar functionality. The second advantage I've seen from `hvplot` is the use of interactive backends. `hvplot` uses `bokeh` as the default plotting engine, which has built in interactivity -- in particular, it's very helpful for me to hover over a map and see the exact values (lon, lat, and color) at the pixel that my cursor is hovering over.

Here's an example how the syntax looks.

In the most simple case, we can just replace `.plot()` with `.hvplot()`!
```{python}
import hvplot.xarray

fig_eg1 = eg_xr.mean('time').hvplot() 
```

If we want the same figure using `matplotlib`, then it's a one-liner

```{python}
hvplot.extension('matplotlib')
fig_eg1 # now it should show as a matplotlib figure.

# changing back to bokeh (the default)
hvplot.extension('bokeh')
```

Here's a more complicated plot that shows off a bunch of options

```{python}

fig_eg2 = eg_xr.hvplot(
    projection=ccrs.PlateCarree(),
    coastline=True,
    cmap=reds_discrete, # discrete colorbar
    clim=(0, 15), # colorbar limits
    title="example title",
    clabel="", # colorbar title
    xlabel="",
    ylabel="",
).opts(fontscale=2.5, ylim=(-60, None)) # changing fontsize and removing antarctica

```


Now combining figures!

```{python}
fig_eg1 + fig_eg2 # side by side


(fig_eg1 + fig_eg2).cols(1) # two rows, 1 column

```

For making paper-ready figures, sometimes it's hard to get the figure to look the way you want it to look just using the `hvplot` options. Their documentation could definitely use some work. That's why I mainly recommend hvplot for prototyping figures. For final figures, it might be better to start from scratch using matploblib.

If you don't want to start completely from scratch, it's not too hard to get the underlying matplotlib objects from hvplot. Here's what I mean:

```{python}

# "convert" the plot from hvplot -> matplotlib
renderer = hv.renderer('matplotlib')
hv_plot_obj = renderer.get_plot(fig_eg2)

# get the usual matplotlib objects
fig = hv_plot_obj.figure
ax = hv_plot_obj.handles['ax']
cax = hv_plot_obj.handles['cax'] # colorbar

# and now we can do normal matplotlib things
ax.set_title('new title using matplotlib')

```





# Regression

## Separate regressions for each gridcell

In @sec-detrending, we saw how to regress a variable against time. Here, we consider regressing one variable against another.

For our example, we're going to regress CESM temperature against GISTEMP temperature. Note that we're using the re-gridded GISTEMP data, so that the two rasters are on the same grid

::: {.panel-tabset group="language"}

## R 

If all we're interested in is the slope, then the `regress` function works well and is fast. But... if we want anything else (e.g. p-values, out of sample predictions), then the `regress` function isn't very useful.
```{r}
##  terra

# method 1: if all we care about is the slope
cesm_gistemp <- regress(cesm_terra, gistemp_terra, formula = y ~ x)
cesm_gistemp$x # a map with the regression slope

# method 2: if we need other regression output (or a different type of regression)
x <- lapp(sds(p, r), \(x, y) {
    sapply(1:nrow(x), \(i) {
        coefficients(lm(b~a, data=data.frame(a=x[i,], b=y[i,])))
        }) |> t()
    })


```

## Python

```{python}

# todo: clean up with example. code gist belongs to karen!

def regression_slope(x, y):
    """
    Get the slope dy/dx, removing missing data points
    """
    # Mask pairs where either x or y is NaN
    mask = np.isfinite(x) & np.isfinite(y)
    if mask.sum() > 1:  # Need at least two points to fit
        fit = linregress(x[mask], y[mask])
        return fit.slope
    else:
        return np.nan  # Return NaN if there are not enough valid data points
    
    

slopes = xr.apply_ufunc(
    regression_slope,
    x,
    y,
    input_core_dims=[['year'], ['year']],
    vectorize=True,
    dask='parallelized',  # Use parallelization if data is large and dask-backed
    output_dtypes=[float]
)

```

:::

<!-- # Conversion between data types

We commonly represent spatiotemporal data cubes as matrices: one row per time point, and one column per spatial gridcell. How do we convert from our spatial objects (i.e. xarray, spatRaster) to this matrix form?

::: {.panel-tabset group="language"}

## R


```{r}
# terra -> matrix
mat_terra <- values(cesm_terra)  # each row is a gridcell, each column is a timepoint
t(mat_terra)  # n_time * n_gridcell representation

```



## Python

```{python}
# todo
```


:::
 -->





```{r}
#| code-fold: true
#| code-summary: Bonus! in R, how do we convert between different spatial packages?
# terra -> stars
terra_to_stars <- st_as_stars(stars_to_terra)


# stars -> terra

# note: this only works if cesm_stars is *not* a stars proxy object!
stars_to_terra <- as(cesm_stars, 'SpatRaster')

# tidync -> stars


  st_as_stars.tidync <- function(x, ...) {
  ## x is a tidync
  
  ## ignore unit details for the moment
  data <- lapply(tidync::hyper_array(x, drop = FALSE), 
                 units::as_units)
  ## this needs to be a bit easier ...
  transforms <- tidync:::active_axis_transforms(x)
  dims <- lapply(names(transforms), function(trname) {
    transform <- transforms[[trname]] %>% dplyr::filter(selected)
    values <- transform[[trname]]
    if (length(values) > 1) {
      stars:::create_dimension(
        values = values)
    } else {
      ## a hack for now when there's only one value
      structure(list(from = values, to = values, 
                     offset = values, delta = NA_real_, 
                     geotransform = rep(NA_real_, 6), 
                     refsys = NA_character_, 
                     point = NA, 
                     values = NULL), 
                class = "dimension")
    }
  })
  names(dims) <- names(transforms)
  if (length(transforms)>= 2L) {
    r <- structure(list(affine = c(0, 0), 
                 dimensions = names(dims)[1:2], 
                 curvilinear = FALSE, class = "stars_raster"))
  
    attr(dims, "raster") <- r
}  
  geotransform_xy <- c(dims[[1]]$offset, dims[[1]]$delta, 0, dims[[2]]$offset, 0, dims[[2]]$delta)
  dims[[1]]$geotransform <- dims[[2]]$geotransform <- geotransform_xy
  structure(data, dimensions =   structure(dims, class = "dimensions"), 
            class = "stars")
  
}

# tnc_stars <- stars::st_as_stars(tnc_tib, dims = c('lon', 'lat', 'time'))
tnc_stars <- st_as_stars.tidync(cesm_tnc)


## stars implements mutate by going stars -> df -> dplyr::mutate -> stars. Here's how that looks.

# stars to df
stars_df <- stars:::to_df(cesm_stars) 

# df BACK to stars (i.e., assumes that we started with a stars object called cesm_stars)
df_stars <- stars_df %>%
  set_dim(dim(cesm_stars)) %>%
  st_as_stars(dimensions = st_dimensions(cesm_stars))

# terra -> df

tidyterra::as_tibble(cesm_terra, xy = T)

```

