---
title: "Where to get climate data?"
format: html
execute: 
  eval: false
---


# CMIP

::: {.panel-tabset group="language"}


## R

```{r}
# pak::pkg_install('eliocamp/rcmip6')       # for nice CMIP access
library(tidyverse)
library(rcmip6) # pak::pkg_install("eliocamp/rcmip6")
source(here::here('rcmip6_fixes.R'))


tas_query <- list(
  type = "Dataset",
  replica = "false",
  latest = "true",
  variable_id = "tas",
  project = "CMIP6",
  frequency = "mon",
  table_id = "Amon",
  experiment_id = "historical",
  source_id = "CanESM5"
)


query_rss_canesm5 <- list(
  type = "Dataset",
  replica = "false",
  latest = "true",
  variable_id = "rss",
  project = "CMIP6",
  frequency = "mon",
  experiment_id = "historical",
  source_id = "CanESM5"
)


results_rss_canesm5 <- cmip_search(
  query_rss_canesm5,
  url = "https://aims2.llnl.gov/proxy/search"
)
results_rss_canesm5_filtered <- results_rss_canesm5 %>%
  cmip_simplify() |> # To keep only the most informative columns
  subset(, select = -full_info) |>
  as_tibble() %>%
  distinct(member_id, .keep_all = T) %>%
  filter(str_detect(member_id, 'p1f1'))


cmip_urls(results_rss_canesm5_filtered)
cmip_download(results_rss_canesm5_filtered)

cmip_info(results_rss_canesm5)
results_rss_canesm5_filtered |>
  cmip_simplify() |> # To keep only the most informative columns
  mutate(ens_number = as.numeric(str_extract(member_id, "(?<=r)\\d+"))) %>%
  arrange(ens_number)


var_list <- c("rss", "tas")
# list of cmip6 models with large ensembles
source_df <- tribble(
  ~source_id, ~n_members,
  'CanESM5',
  25,
  'ACCESS-ESM1-5',
  30,
  'IPSL-CM6A-LR',
  32,
  'MIROC-ES2L',
  31,
  'MIROC6',
  50,
  'CNRM-CM6-1',
  29,
  'EC-Earth3',
  22,
  'NorCPM1',
  30
)



tribble(
  ~col1, ~ col2,
  1, 2
  3, 3
)


#' @param var should use cmip variable_id.
get_cmip6_monthly_le <- function(source = 'CANESM5', var = 'tas') {
  query <- list(
    type = "Dataset",
    replica = "false",
    latest = "true",
    variable_id = var,
    project = "CMIP6",
    frequency = "mon",
    experiment_id = "historical",
    source_id = source
  )
  results <- cmip_search(
    query,
    url = "https://aims2.llnl.gov/proxy/search"
  )
  results_filtered <- results %>%
    cmip_simplify() |> # To keep only the most informative columns
    subset(, select = -full_info) |>
    as_tibble() %>%
    distinct(member_id, .keep_all = T) #%>%
  #filter(str_detect(member_id, 'p1f1'))

  results_filtered
}


identical(
  nrow(results),
  source_df %>%
    filter(source_id == distinct(results$source_id)) %>%
    pull(n_members)
)


```

## Python

```{python}
#| code-fold: true

import intake
import xarray as xr
from tqdm import tqdm
import pyarrow as pa
import polars as pl
import numpy as np
import os.path


#### change the next three lines
var_to_use = ["rsus", "rsds", "tas"]
exps = ["historical"]
savedir = "D:data\\CMIP6\\processed_data\\"
#########################################

# Open catalog
col_url = "https://storage.googleapis.com/cmip6/pangeo-cmip6.json"
col = intake.open_esm_datastore(col_url)

# Search for the variable and experiments
# might need to change the table_id, depending on the variables
cat = col.search(variable_id=var_to_use, table_id="Amon", experiment_id=exps)

# Work with a local copy of the catalog dataframe
df = cat.df.dropna(subset=["zstore"]).copy()

# Create a combined ID of model and member
df["source_member"] = df["source_id"].astype(str) + "." + df["member_id"].astype(str)

# Keep only model/member pairs that have all variables in both experiments
# can also add experiments to the groupby
counts = df.groupby(["source_member", "variable_id"]).size().unstack()
valid_sources = counts.dropna().index.tolist()
df = df[df["source_member"].isin(valid_sources)]


# Keep only members that have the "standard forcings" (f1), and the first set of physics (p1) ----
# this part is optional. Can replace with any other subsetting you want to do.
is_p1f1 = df["member_id"].str.endswith("p1f1")
df = df[is_p1f1]

# remove duplicates
df = df.drop_duplicates(
    subset=["source_id", "member_id", "experiment_id", "variable_id"]
)

# Keep only source_ids that have at least 20 ensemble members
# this part is optional. Can replace with any other subsetting you want to do.
ens_member_counts = (
    df.groupby(["source_id", "variable_id"]).size().reset_index(name="count")
)
min_n_members = ens_member_counts.groupby("source_id")["count"].min()
sources_to_keep = min_n_members[min_n_members >= 20].index.tolist()
df = df[df["source_id"].isin(sources_to_keep)]

# View example keys (you can check one with df.iloc[0])
print(df)

# Group by model/member
grouped = df.groupby(["source_id", "member_id"])

## go through each model/member and save to file.

# next(iter(grouped)) # look at the first group
for (model, member), group_df in tqdm(grouped, desc="Processing models"):
    if os.path.exists(f"{savedir}monthly_rss_tas_{member}_{model}.nc"):
        # skip if file already exists
        continue
    try:
        # get paths for all of the different variables
        zstores = dict(zip(group_df["variable_id"], group_df["zstore"]))
        if not set(var_to_use).issubset(zstores):
            continue  # skip incomplete

        # load all of the variables
        ds = xr.open_mfdataset(
            list(zstores.values()), engine="zarr", combine="by_coords", join="override"
        )
        
        # can do some preprocessing to ds here.

        # save to file. replace with desired filename.
        ds.to_netcdf(f"{savedir}monthly_rss_tas_{member}_{model}.nc")

    except Exception as e:
        print(f"Skipped {model}_{member}: {e}")

```

:::


# CESM specifically


# ERA

ecmwfr is an R API wrapper to download ERA data. To use this package, you'll need to head over to the ECMWF website, make an account, and get your API key. See instructions on the ecmwfr package README (the code chunk below is directly copied from that readme.)

Note: the general workflow is: go to the "climate data store", use their gui to filter files that you want, grab the api call (the cilmate data store has a big button for you to click on to copy it), then paste that text into R.  
The original ecmwfr package had a RStudio add-on that would convert that copied text into an R list, but since I'm not using RStudio anymore, I've instead modified their function that takes text -> list. This is `python_to_list()` below.

```{r}
#| eval: false

# pak::pkg_install('bluegreen-labs/ecmwfr') # for better ERA5 data access
library(ecmwfr)

# set a key to the keychain -- you'll only have to do this once!
# wf_set_key(key = "abcd1234-foo-bar-98765431-XXXXXXXXXX")

# fixing ecmwfr:::python_to_list so that it works without the add in.
python_to_list <- function(python_text) {

  if(missing(python_text)){
    stop("No input text provided")
  }

  # substitute inconsistent quotes, sorting the f-ing API mess
  # for consistency
  python_text <- gsub("\\\"","\'", python_text)

  # split string on line breaks
  line_breaks <- strsplit(python_text, "\\n")[[1]]

  leading <- line_breaks[grep("dataset =", line_breaks)]
  trailing <- line_breaks[grep("target =", line_breaks)]

  # clean up leading and trailing ends if any
  if (nchar(leading) == 0 || length(leading) == 0){
    stop("Incomplete query selection: dataset is missing")
  }

  # dropping field name and removing quotes
  leading <- gsub("dataset = ","", leading)
  leading <- gsub("\'","", leading)

  # clean up leading and trailing ends if any
  if (nchar(trailing) == 0 || length(trailing) == 0){
    trailing <- "TMPFILE"
  } else {
    # dropping field name and removing quotes
    trailing <- gsub("target = ","", trailing)
    trailing <- gsub("\'","", trailing)
  }

  # grab anything between {} within ()
  c_brackets <- try(gsub("[\\{\\}]", "",
                         regmatches(python_text,
                                    gregexpr("\\{.*?\\}", python_text))[[1]]))

  # check if strings have content
  if(length(c_brackets) == 0){
    stop("Incomplete query selection")
  }

  # trap the goddamn inconsistent MARS json formatting issue
  c_brackets <- gsub("\n| ","", c_brackets)
  if(substr(c_brackets, nchar(c_brackets),nchar(c_brackets)) == ","){
    c_brackets <- substr(c_brackets, 1, nchar(c_brackets)-1)
  }

  # Remove trailing commas (",]")
  c_brackets <- gsub(",]", "]", c_brackets)
  c_brackets <- paste0("{",c_brackets,"}")
  c_brackets <- gsub("\'", "\"", c_brackets)

  # read in data as list
  c_list <- jsonlite::fromJSON(c_brackets)

  # add leading and trailing bits
  # to request list
  c_list <- c(dataset_short_name = leading, c_list, target = trailing)

  # clean up leading and trailing ends if any
  if (nchar(leading) == 0 || length(leading) == 0){
    c_list$dataset_short_name <- gsub('\\n|,\\n| |"','', leading)
  }

  if (nchar(trailing) == 0 || length(trailing) == 0){
    c_list$target <- gsub('\\n|,\\n| |"','', trailing)
  }

  # clean up list values
  list_values <- lapply(c_list, function(l){
    if(length(l) == 1){
      paste0('"',l,'"')
    } else {
      l
    }
  })

  # returned cleaned up list

  c_list


}

request <- python_to_list(
  'dataset = "reanalysis-era5-single-levels-monthly-means"
request = {
    "product_type": ["monthly_averaged_reanalysis"],
    "variable": ["2m_temperature"],
    "year": ["2024"],
    "month": [
        "01", "02", "03",
        "04", "05", "06",
        "07", "08", "09",
        "10", "11", "12"
    ],
    "time": ["00:00"],
    "data_format": "netcdf",
    "download_format": "unarchived"
}
') 

# If you have stored your user login information
# in the keyring by calling cds_set_key you can
# call:
file <- wf_request(
 request  = request,  # the request
 transfer = TRUE,     # download the file
 path     = here()      # store data in current working directory
 )


era_path <- here('era5-demo.nc')
```

Now, era5-demo.nc should be downloaded into your working directory!


# NASA GISSTEMP

This is their land temperature product. Using good old fashioned base R capabilities.
```{r}
#| eval: false
#| 
gisstemp_url <- 'https://data.giss.nasa.gov/pub/gistemp/gistemp250_GHCNv4.nc.gz'

# downloads into the working directory
download.file(gisstemp_url, destfile = here('ghcn_demo.nc.gz'))

# unzip the file
R.utils::gunzip(here('ghcn_demo.nc.gz'), remove = FALSE)


ghcn_path <- here('ghcn_demo.nc')
```


# misc

ClimateR is a cool package for pulling out data from a point (or a general area)

```{r}
#| eval: false

# climateR demo
```



geodata is a nice source for simplified, smaller versions of data. Super easy to pull.
```{r}
#| eval: false

# # example data (picked because of ease of access and small size)
# WARNING: will download to current directory
library(geodata)
eg_res10 <- cmip6_world(
  model = 'CanESM5', ssp = '585', 
  time ='2021-2040', var = 'tavg',
  res = 10, path = here()
  )

eg_res5 <- cmip6_world('CanESM5')
```